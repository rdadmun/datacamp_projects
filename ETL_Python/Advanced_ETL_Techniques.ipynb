{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingesting JSON data with pandas\n",
    "When developing a data pipeline, you may have to work with non-tabular data and data sources, such as APIs or JSON files. In this exercise, we'll practice extracting data from a JSON file using pandas.\n",
    "\n",
    "pandas has been imported as pd, and the JSON file you'll ingest is stored at the path \"testing_scores.json\".\n",
    "\n",
    "Update the extract() function read a JSON file into a pandas DataFrame, orienting by records.\n",
    "Pass the path testing_scores.json to the extract() function, and store the output to a variable called raw_testing_scores.\n",
    "Print the head of the raw_testing_scores DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(file_path):\n",
    "  # Read the JSON file into a DataFrame\n",
    "  return pd.read_json(file_path, orient=\"records\")\n",
    "\n",
    "# Call the extract function with the appropriate path, assign to raw_testing_scores\n",
    "raw_testing_scores = extract(\"testing_scores.json\")\n",
    "\n",
    "# Output the head of the DataFrame\n",
    "print(raw_testing_scores.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading JSON data into memory\n",
    "When data is stored in JSON format, it's not always easy to load into a DataFrame. This is the case for the \"nested_testing_scores.json\" file. Here, the data will have to be manually manipulated before it can be stored in a DataFrame.\n",
    "\n",
    "To help get you started, pandas has been loaded into the workspace as pd.\n",
    "\n",
    "Use pandas to read a JSON file into a DataFrame.\n",
    "Pass the \"nested_scores.json\" file path to the extract() function.\n",
    "\n",
    "Import the json library.\n",
    "Use the json library to load the \"nested_scores.json\" file into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(file_path):\n",
    "  \t# Read the JSON file into a DataFrame, orient by index\n",
    "\treturn pd.read_json(file_path, orient=\"index\")\n",
    "\n",
    "# Call the extract function, pass in the desired file_path\n",
    "raw_testing_scores = extract(\"nested_scores.json\")\n",
    "print(raw_testing_scores.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating over dictionaries\n",
    "Once JSON data is loaded into a dictionary, you can leverage Python's built-in tools to iterate over its keys and values.\n",
    "\n",
    "The \"nested_school_scores.json\" file has been read into a dictionary stored in the raw_testing_scores variable, which takes the following form:\n",
    "\n",
    "The .items() method can be called on a dictionary to iterate through both the keys and values of a dictionary in a single line of code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_testing_scores_keys = []\n",
    "\n",
    "# Iterate through the keys of the raw_testing_scores dictionary\n",
    "for school_id in raw_testing_scores.keys():\n",
    "  \t# Append each key to the raw_testing_scores_keys list\n",
    "\traw_testing_scores_keys.append(school_id)\n",
    "    \n",
    "print(raw_testing_scores_keys[0:3]);\n",
    "\n",
    "raw_testing_scores_keys = []\n",
    "raw_testing_scores_values = []\n",
    "\n",
    "# Iterate through the values of the raw_testing_scores dictionary\n",
    "for school_id, school_info in raw_testing_scores.items():\n",
    "\traw_testing_scores_keys.append(school_id)\n",
    "\traw_testing_scores_values.append(school_info)\n",
    "\n",
    "print(raw_testing_scores_keys[0:3])\n",
    "print(raw_testing_scores_values[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing data from dictionaries\n",
    "When JSON data is loaded into memory, the resulting dictionary can be complicated. Key-value pairs may contain another dictionary, such are called nested dictionaries. These nested dictionaries are frequently encountered when dealing with APIs or other JSON data. In this exercise, you will practice extracting data from nested dictionaries and handling missing values.\n",
    "\n",
    "The dictionary below is stored in the school variable. Good luck!\n",
    "\n",
    "{\n",
    "    \"street_address\": \"111 Columbia Street\",\n",
    "    \"city\": \"Manhattan\",\n",
    "    \"scores\": {\n",
    "        \"math\": 657,\n",
    "        \"reading\": 601\n",
    "    }\n",
    "}\n",
    "\n",
    "Parse the value stored at the \"street_address\" key from the school dictionary.\n",
    "Parse the value stored at the \"scores\" key from the school dictionary.\n",
    "Parse the values stored at the \"math\", \"reading\", and \"writing\" keys from the scores dictionary, and set the default value to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the street_address from the dictionary\n",
    "street_address = school.get(\"street_address\")\n",
    "\n",
    "# Parse the scores dictionary\n",
    "scores = school.get(\"scores\")\n",
    "\n",
    "# Try to parse the math, reading and writing values from scores\n",
    "math_score = scores.get(\"math\", 1)\n",
    "reading_score = scores.get(\"reading\",1)\n",
    "writing_score = 0\n",
    "\n",
    "print(f\"Street Address: {street_address}\")\n",
    "print(f\"Math: {math_score}, Reading: {reading_score}, Writing: {writing_score}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming JSON data\n",
    "Chances are, when reading data from JSON format into a dictionary, you'll probably have to apply some level of manual transformation to the data before it can be stored in a DataFrame. This is common when working with nested dictionaries, which you'll have the opportunity to explore in this exercise.\n",
    "\n",
    "The \"nested_school_scores.json\" file has been read into a dictionary available in the raw_testing_scores variable, which takes the following form:\n",
    "\n",
    "{\n",
    "    \"01M539\": {\n",
    "        \"street_address\": \"111 Columbia Street\",\n",
    "        \"city\": \"Manhattan\",\n",
    "        \"scores\": {\n",
    "              \"math\": 657,\n",
    "              \"reading\": 601,\n",
    "              \"writing\": 601\n",
    "        }\n",
    "  }, ...\n",
    "}\n",
    "\n",
    "Loop through both the keys and values of the raw_testing_scores dictionary.\n",
    "Extract the \"street_address\" from each dictionary nested in the raw_testing_scores object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_testing_scores = []\n",
    "\n",
    "# Loop through each of the dictionary key-value pairs\n",
    "for school_id, school_info in raw_testing_scores.items():\n",
    "\tnormalized_testing_scores.append([\n",
    "    \tschool_id,\n",
    "    \tschool_info.get(\"street_address\"),  # Pull the \"street_address\"\n",
    "    \tschool_info.get(\"city\"),\n",
    "    \tschool_info.get(\"scores\").get(\"math\", 0),\n",
    "    \tschool_info.get(\"scores\").get(\"reading\", 0),\n",
    "    \tschool_info.get(\"scores\").get(\"writing\", 0),\n",
    "    ])\n",
    "\n",
    "print(normalized_testing_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming and cleaning DataFrames\n",
    "Once data has been curated into a cleaned Python data structure, such as a list of lists, it's easy to convert this into a pandas DataFrame. You'll practice doing just this with the data that was curated in the last exercise.\n",
    "\n",
    "Per usual, pandas has been imported as pd, and the normalized_testing_scores variable stores the list of each schools testing data, as shown below.\n",
    "\n",
    "[\n",
    "    ['01M539', '111 Columbia Street', 'Manhattan', 657.0, 601.0, 601.0],\n",
    "    ...\n",
    "]   \n",
    "\n",
    "\n",
    "Create a pandas DataFrame from the list of lists stored in the normalized_testing_scores variable.\n",
    "Set the columns names for the normalized_data DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the normalized_testing_scores list\n",
    "normalized_data = pd.DataFrame(normalized_testing_scores)\n",
    "\n",
    "# Set the column names\n",
    "normalized_data.columns = [\"school_id\", \"street_address\", \"city\", \"avg_score_math\", \"avg_score_reading\", \"avg_score_writing\"]\n",
    "\n",
    "normalized_data = normalized_data.set_index(\"school_id\")\n",
    "print(normalized_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling missing values with pandas\n",
    "When building data pipelines, it's inevitable that you'll stumble upon missing data. In some cases, you may want to remove these records from the dataset. But in others, you'll need to impute values for the missing information. In this exercise, you'll practice using pandas to impute missing test scores.\n",
    "\n",
    "Data from the file \"testing_scores.json\" has been read into a DataFrame, and is stored in the variable raw_testing_scores. In addition to this, pandas has been loaded as pd.\n",
    "\n",
    "Print the head of the raw_testing_scores DataFrame, and observe the NaN values.\n",
    "Use the average of the \"math_score\" column to fill the NaN values in the \"math_score\" column.\n",
    "Print the head of the updated DataFrame.\n",
    "\n",
    "For the \"math_score\", \"reading_score\" and \"writing_score\" columns, update the transform() function to fill NaN values with the mean of the respective columns, in place.\n",
    "Print the head of the cleaned DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the head of the `raw_testing_scores` DataFrame\n",
    "print(raw_testing_scores.head())\n",
    "\n",
    "\n",
    "# Fill NaN values with the average from that column\n",
    "raw_testing_scores[\"math_score\"] = raw_testing_scores[\"math_score\"].fillna(raw_testing_scores[\"math_score\"].mean())\n",
    "\n",
    "# Print the head of the raw_testing_scores DataFrame\n",
    "print(raw_testing_scores.head())\n",
    "\n",
    "def transform(raw_data):\n",
    "\traw_data.fillna(\n",
    "    \tvalue={\n",
    "\t\t\t# Fill NaN values with column mean\n",
    "\t\t\t\"math_score\": raw_data[\"math_score\"].mean(),\n",
    "\t\t\t\"reading_score\": raw_data[\"reading_score\"].mean(),\n",
    "\t\t\t\"writing_score\": raw_data[\"writing_score\"].mean()\n",
    "\t\t}, inplace=True\n",
    "\t)\n",
    "\treturn raw_data\n",
    "\n",
    "clean_testing_scores = transform(raw_testing_scores)\n",
    "\n",
    "# Print the head of the clean_testing_scores DataFrame\n",
    "print(clean_testing_scores.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping data with pandas\n",
    "The output of a data pipeline is typically a \"modeled\" dataset. This dataset provides data consumers easy access to information, without having to perform much manipulation. Grouping data with pandas helps to build modeled datasets,\n",
    "\n",
    "pandas has been imported as pd, and the raw_testing_scores DataFrame contains data in the following form:\n",
    "\n",
    "              street_address       city  math_score  reading_score  writing_score\n",
    "01M539   111 Columbia Street  Manhattan       657.0          601.0          601.0\n",
    "02M294      350 Grand Street  Manhattan       395.0          411.0          387.0\n",
    "02M308      350 Grand Street  Manhattan       418.0          428.0          415.0\n",
    "\n",
    "Use .loc[] to only keep the \"city\", \"math_score\", \"reading_score\", and \"writing_score\" columns.\n",
    "Group the DataFrame by the \"city\" column, and find the mean of each city's math, reading, and writing scores.\n",
    "Use the transform() function to create a grouped DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(raw_data):\n",
    "\t# Use .loc[] to only return the needed columns\n",
    "\traw_data = raw_data.loc[:, [\"city\", \"math_score\", \"reading_score\", \"writing_score\"]]\n",
    "\t\n",
    "    # Group the data by city, return the grouped DataFrame\n",
    "\tgrouped_data = raw_data.groupby(by=[\"city\"], axis=0).mean()\n",
    "\treturn grouped_data\n",
    "\n",
    "# Transform the data, print the head of the DataFrame\n",
    "grouped_testing_scores = transform(raw_testing_scores)\n",
    "print(grouped_testing_scores.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying advanced transformations to DataFrames\n",
    "pandas has a plethora of built-in transformation tools, but sometimes, more advanced logic needs to be used in a transformation. The apply function lets you apply a user-defined function to a row or column of a DataFrame, opening the door for advanced transformation and feature generation.\n",
    "\n",
    "The find_street_name() function parses the street name from the \"street_address\", dropping the street number from the string. This function has been loaded into memory, and is ready to be applied to the raw_testing_scores DataFrame.\n",
    "\n",
    "In the definition of the transform() function, use the find_street_name() function to create a new column with the name \"street_name\".\n",
    "Use the transform() function to clean the raw_testing_scores DataFrame.\n",
    "Print the head of the cleaned_testing_scores DataFrame, observing the new \"street_name\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(raw_data):\n",
    "\t# Use the apply function to extract the street_name from the street_address\n",
    "    raw_data[\"street_name\"] = raw_data.apply(\n",
    "   \t\t# Pass the correct function to the apply method\n",
    "        find_street_name,\n",
    "        axis=1\n",
    "    )\n",
    "    return raw_data\n",
    "\n",
    "# Transform the raw_testing_scores DataFrame\n",
    "cleaned_testing_scores = transform(raw_testing_scores)\n",
    "\n",
    "# Print the head of the cleaned_testing_scores DataFrame\n",
    "print(cleaned_testing_scores.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data to a Postgres database\n",
    "After data has been extracted from a source system and transformed to align with analytics or reporting use cases, it's time to load the data to a final storage medium. Storing cleaned data in a SQL database makes it simple for data consumers to access and run queries against. In this example, you'll practice loading cleaned data to a Postgres database.\n",
    "\n",
    "sqlalchemy has been imported, and pandas is available as pd. The first few rows of the cleaned_testing_scores DataFrame are shown below:\n",
    "\n",
    "Update the connection string to write to the schools database and create a connection object using sqlalchemy.\n",
    "Use pandas to write the cleaned_testing_scores DataFrame to the scores table in the schools database.\n",
    "If the table is already populated with data, make sure to replace the values with the current DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the connection string, create the connection object to the schools database\n",
    "db_engine = sqlalchemy.create_engine(\"postgresql+psycopg2://repl:password@localhost:5432/schools\")\n",
    "\n",
    "# Write the DataFrame to the scores table\n",
    "cleaned_testing_scores.to_sql(\n",
    "\tname=\"scores\",\n",
    "\tcon=db_engine,\n",
    "\tindex=False,\n",
    "\tif_exists=\"replace\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validating data loaded to a Postgres Database\n",
    "In this exercise, you'll finally get to build a data pipeline from end-to-end. This pipeline will extract school testing scores from a JSON file and transform the data to drop rows with missing scores. In addition to this, each will be ranked by the city they are located in, based on their total scores. Finally, the transformed dataset will be stored in a Postgres database.\n",
    "\n",
    "To give you a head start, the extract() and transform() functions have been built and used as shown below. In addition to this, pandas has been imported as pd. Best of luck!\n",
    "\n",
    "# Extract and clean the testing scores.\n",
    "raw_testing_scores = extract(\"testing_scores.json\")\n",
    "cleaned_testing_scores = transform(raw_testing_scores)\n",
    "\n",
    "Update the load() function to write the clean_data DataFrame to the scores_by_city table in the schools database.\n",
    "If data exists in the scores_by_city table, makes sure to replace it with the updated data.\n",
    "\n",
    "Load the data from the cleaned_testing_scores, using the db_engine that has already been defined.\n",
    "Use pandas to read data from the scores_by_city table, and print the first few rows of the DataFrame to validate that data was persisted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(clean_data, con_engine):\n",
    "\t# Store the data in the schools database\n",
    "    clean_data.to_sql(\n",
    "    \tname=\"scores_by_city\",\n",
    "\t\tcon=con_engine,\n",
    "\t\tif_exists=\"replace\",  # Make sure to replace existing data\n",
    "\t\tindex=True,\n",
    "\t\tindex_label=\"school_id\"\n",
    "    )\n",
    "\n",
    "def load(clean_data, con_engine):\n",
    "    clean_data.to_sql(name=\"scores_by_city\", con=con_engine, if_exists=\"replace\", index=True, index_label=\"school_id\")\n",
    "    \n",
    "# Call the load function, passing in the cleaned DataFrame\n",
    "load(cleaned_testing_scores, db_engine)\n",
    "\n",
    "# Call query the data in the scores_by_city table, check the head of the DataFrame\n",
    "to_validate = pd.read_sql(\"SELECT * FROM scores_by_city\", con=db_engine)\n",
    "print(to_validate.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
